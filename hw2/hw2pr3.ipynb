{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(X, y, reg=0.0):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\n",
    "\t\tReturn the optimal weight matrix.\n",
    "\t'''\n",
    "\t# Hint: Find the numerical solution for part c\n",
    "\t# Use np.eye to create identity matrix\n",
    "\t# Use np.linalg.solve to solve W_opt\n",
    "\n",
    "\teye = np.eye(X.shape[1])\n",
    "\teye[0, 0] = 0 # don't regularize the bias term\n",
    "\tW_opt = np.linalg.solve(X.T @ X + reg * eye, X.T @ y)\n",
    "\treturn W_opt\n",
    "\n",
    "def predict(W, X):\n",
    "\t'''\n",
    "\t\tW is a weight matrix with bias.\n",
    "\t\tX is the data with dimension m x (n + 1).\n",
    "\n",
    "\t\tReturn the predicted label, y_pred.\n",
    "\t'''\n",
    "\treturn X * W\n",
    "\n",
    "def find_RMSE(W, X, y):\n",
    "\t'''\n",
    "\t\tW is the weight matrix with bias.\n",
    "\t\tX is the data with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\n",
    "\t\tReturn the root mean-squared error.\n",
    "\t'''\n",
    "\ty_pred = predict(W, X)\n",
    "\tdiff = y - y_pred\n",
    "\tm = X.shape[0]\n",
    "\tMSE = np.linalg.norm(diff, 2) ** 2 / m\n",
    "\treturn np.sqrt(MSE)\n",
    "\n",
    "def RMSE_vs_lambda(X_train, y_train, X_val, y_val):\n",
    "\t'''\n",
    "\t\tX is the data with dimension m x (n + 1).\n",
    "\t\ty is the label with dimension m x 1.\n",
    "\n",
    "\t\tGenearte a plot of RMSE vs lambda.\n",
    "\t\tReturn the regularization parameter that minimizes RMSE.\n",
    "\t'''\n",
    "\t# Set up plot style\n",
    "\tplt.style.use('ggplot')\n",
    "\n",
    "\tRMSE_list = []\n",
    " \n",
    "\t# Construct a list of regularization parameters with random uniform sampling\n",
    "\t# Then, generate a list of W_opt's according to these parameters\n",
    "\t# Finally, generate a list of RMSE according to reg_list\n",
    "\treg_list = np.random.uniform(0.0, 150.0, 150)\n",
    "\treg_list.sort()\n",
    "\tW_list = [linreg(X_train, y_train, reg = lb) for lb in reg_list]\n",
    "\tfor index in range(len(reg_list)):\n",
    "\t\tW_opt = W_list[index]\n",
    "\t\tRMSE_list.append(find_RMSE(W_opt, X_val, y_val))\n",
    "\n",
    "\t# Plot RMSE vs lambda\n",
    "\tRMSE_vs_lambda_plot, = plt.plot(reg_list, RMSE_list)\n",
    "\tplt.setp(RMSE_vs_lambda_plot, color = 'red')\n",
    "\tplt.title('RMSE vs lambda')\n",
    "\tplt.xlabel('lambda')\n",
    "\tplt.ylabel('RMSE')\n",
    "\tplt.savefig('RMSE_vs_lambda.png', format = 'png')\n",
    "\tplt.close()\n",
    "\tprint('==> Plotting completed.')\n",
    "\n",
    "\t# Find the regularization value that minimizes RMSE\n",
    "\topt_lambda_index = np.argmin(RMSE_list)\n",
    "\treg_opt = reg_list[opt_lambda_index]\n",
    "\treturn reg_opt\n",
    "\n",
    "def norm_vs_lambda(X_train, y_train, X_val, y_val):\n",
    "\t'''\n",
    "\t\tX is the data with dimension m x (n + 1).\n",
    "\t\ty is the label with dimension m x 1.\n",
    "\n",
    "\t\tGenearte a plot of norm of the weights vs lambda.\n",
    "\t'''\n",
    "\t# You may reuse the code for RMSE_vs_lambda\n",
    "\t# to generate the list of weights and regularization parameters\n",
    "\treg_list = np.random.uniform(0.0, 150.0, 150)\n",
    "\treg_list.sort()\n",
    "\tW_list = [linreg(X_train, y_train, reg = lb) for lb in reg_list]\n",
    "\n",
    "\t# Calculate the norm of each weight\n",
    "\tnorm_list = [np.linalg.norm(W, 2) for W in W_list]\n",
    "\n",
    "\t# Plot norm vs lambda\n",
    "\tnorm_vs_lambda_plot, = plt.plot(reg_list, norm_list)\n",
    "\tplt.setp(norm_vs_lambda_plot, color = 'blue')\n",
    "\tplt.title('norm vs lambda')\n",
    "\tplt.xlabel('lambda')\n",
    "\tplt.ylabel('norm')\n",
    "\tplt.savefig('norm_vs_lambda.png', format = 'png')\n",
    "\tplt.close()\n",
    "\tprint('==> Plotting completed.')\n",
    "\n",
    "def linreg_no_bias(X, y, reg = 0.0):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x n.\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\n",
    "\t\tReturn the optimal weight and bias separately.\n",
    "\t'''\n",
    "\tt_start = time.time()\n",
    " \n",
    "\t# Find the numerical solution in part d\n",
    "\tm = X.shape[0]\n",
    "\tones = np.eye(m)\n",
    "\tAggregate = X.T @ (np.eye(m) - np.ones(m) / m)\n",
    "\tW_opt = np.linalg.solve(Aggregate @ X + reg * np.eye(Aggregate.shape[0]), \\\n",
    "\t\tAggregate @ y)\n",
    "\tb_opt = sum((y - X @ W_opt)) / m\n",
    " \n",
    "\t# Benchmark report\n",
    "\tt_end = time.time()\n",
    "\tprint('--Time elapsed for training: {t:4.2f} seconds'.format(\\\n",
    "\t\t\t\tt = t_end - t_start))\n",
    "\treturn b_opt, W_opt\n",
    "\n",
    "def grad_descent(X_train, y_train, X_val, y_val, reg = 0.0, \\\n",
    "\tlr_W = 2.5e-12, lr_b = 0.2, max_iter = 150, eps = 1e-6, print_freq = 25):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x n.\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\t\tlr_W is the learning rate for weights.\n",
    "\t\tlr_b is the learning rate for bias.\n",
    "\t\tmax_iter is the maximum number of iterations.\n",
    "\t\teps is the threshold of the norm for the gradients.\n",
    "\t\tprint_freq is the frequency of printing the report.\n",
    "\n",
    "\t\tReturn the optimal weight and bias by gradient descent.\n",
    "\t'''\n",
    "\tm_train, n = X_train.shape\n",
    "\tm_val = X_val.shape[0]\n",
    " \n",
    "\t# initialize the weights and bias and their corresponding gradients\n",
    "\n",
    "\tW = np.zeros((n, 1))\n",
    "\tb = 0.\n",
    "\tW_grad = np.ones_like(W)\n",
    "\tb_grad = 1.\n",
    "\n",
    "\tobj_train = []\n",
    "\tobj_val = []\n",
    "\tprint('==> Running gradient descent...')\n",
    "\titer_num = 0\n",
    "\tt_start = time.time()\n",
    "\n",
    "\twhile np.linalg.norm(W_grad) > eps and np.linalg.norm(b_grad) > eps \\\n",
    "\tand iter_num < max_iter:\n",
    "\t\t# calculate norms\n",
    "\t\ttrain_rmse = np.sqrt(np.linalg.norm((X_train @ W).reshape((-1, 1)) \\\n",
    "\t\t\t+ b - y_train) ** 2 / m_train)\n",
    "\t\tobj_train.append(train_rmse)\n",
    "\t\tval_rmse = np.sqrt(np.linalg.norm((X_val @ W).reshape((-1, 1)) \\\n",
    "\t\t\t+ b - y_val) ** 2 / m_val)\n",
    "\t\tobj_val.append(val_rmse)\n",
    "\t\t# calculate gradient\n",
    "\t\tW_grad = ((X_train.T @ X_train + reg * np.eye(n)) @ W \\\n",
    "\t\t\t+ X_train.T @ (b - y_train)) / m_train\n",
    "\t\tb_grad = (sum(X_train @ W) - sum(y_train) + b * m_train) / m_train\n",
    "\t\t# update weights and bias\n",
    "\t\tW -= lr_W * W_grad\n",
    "\t\tb -= lr_b * b_grad\n",
    "\t\t# print statements\n",
    "\t\tif (iter_num + 1) % print_freq == 0:\n",
    "\t\t\tprint('-- Iteration{} - training rmse {: 4.4f} - gradient norm {: 4.4E}'.format(\\\n",
    "\t\t\t\titer_num + 1, train_rmse, np.linalg.norm(W_grad)))\n",
    "\t\titer_num += 1\n",
    "\n",
    "\t# Benchmark report\n",
    "\tt_end = time.time()\n",
    "\tprint('--Time elapsed for training: {t:4.2f} seconds'.format(\\\n",
    "\t\t\tt = t_end - t_start))\n",
    " \n",
    "\t# generate convergence plot\n",
    "\ttrain_rmse_plot, = plt.plot(range(iter_num), obj_train)\n",
    "\tplt.setp(train_rmse_plot, color = 'red')\n",
    "\tval_rmse_plot, = plt.plot(range(iter_num), obj_val)\n",
    "\tplt.setp(val_rmse_plot, color = 'green')\n",
    "\tplt.legend((train_rmse_plot, val_rmse_plot), \\\n",
    "\t\t('Training RMSE', 'Validation RMSE'), loc = 'best')\n",
    "\tplt.title('RMSE vs iteration')\n",
    "\tplt.xlabel('iteration')\n",
    "\tplt.ylabel('RMSE')\n",
    "\tplt.savefig('convergence.png', format = 'png')\n",
    "\tplt.close()\n",
    "\tprint('==> Plotting completed.')\n",
    "\n",
    "\treturn b, W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "train_pct = 2.0 / 3\n",
    "val_pct = 5.0 / 6\n",
    "df = pd.read_csv('https://math189sp19.github.io/data/online_news_popularity.csv', sep = ', ', engine = 'python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>...</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.350000</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.200000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>-0.187500</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://mashable.com/2013/01/07/ap-samsung-spon...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>255.0</td>\n",
       "      <td>0.604743</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.791946</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.118750</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>-0.100000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://mashable.com/2013/01/07/apple-40-billio...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.575130</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.663866</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.466667</td>\n",
       "      <td>-0.800</td>\n",
       "      <td>-0.133333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://mashable.com/2013/01/07/astronaut-notre...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>531.0</td>\n",
       "      <td>0.503788</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.665635</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.369697</td>\n",
       "      <td>-0.600</td>\n",
       "      <td>-0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://mashable.com/2013/01/07/att-u-verse-apps/</td>\n",
       "      <td>731.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1072.0</td>\n",
       "      <td>0.415646</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.540890</td>\n",
       "      <td>19.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.220192</td>\n",
       "      <td>-0.500</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.136364</td>\n",
       "      <td>505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...      731.0   \n",
       "1  http://mashable.com/2013/01/07/ap-samsung-spon...      731.0   \n",
       "2  http://mashable.com/2013/01/07/apple-40-billio...      731.0   \n",
       "3  http://mashable.com/2013/01/07/astronaut-notre...      731.0   \n",
       "4   http://mashable.com/2013/01/07/att-u-verse-apps/      731.0   \n",
       "\n",
       "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
       "0            12.0             219.0         0.663594               1.0   \n",
       "1             9.0             255.0         0.604743               1.0   \n",
       "2             9.0             211.0         0.575130               1.0   \n",
       "3             9.0             531.0         0.503788               1.0   \n",
       "4            13.0            1072.0         0.415646               1.0   \n",
       "\n",
       "   n_non_stop_unique_tokens  num_hrefs  num_self_hrefs  num_imgs  ...  \\\n",
       "0                  0.815385        4.0             2.0       1.0  ...   \n",
       "1                  0.791946        3.0             1.0       1.0  ...   \n",
       "2                  0.663866        3.0             1.0       1.0  ...   \n",
       "3                  0.665635        9.0             0.0       1.0  ...   \n",
       "4                  0.540890       19.0            19.0      20.0  ...   \n",
       "\n",
       "   min_positive_polarity  max_positive_polarity  avg_negative_polarity  \\\n",
       "0               0.100000                    0.7              -0.350000   \n",
       "1               0.033333                    0.7              -0.118750   \n",
       "2               0.100000                    1.0              -0.466667   \n",
       "3               0.136364                    0.8              -0.369697   \n",
       "4               0.033333                    1.0              -0.220192   \n",
       "\n",
       "   min_negative_polarity  max_negative_polarity  title_subjectivity  \\\n",
       "0                 -0.600              -0.200000            0.500000   \n",
       "1                 -0.125              -0.100000            0.000000   \n",
       "2                 -0.800              -0.133333            0.000000   \n",
       "3                 -0.600              -0.166667            0.000000   \n",
       "4                 -0.500              -0.050000            0.454545   \n",
       "\n",
       "   title_sentiment_polarity  abs_title_subjectivity  \\\n",
       "0                 -0.187500                0.000000   \n",
       "1                  0.000000                0.500000   \n",
       "2                  0.000000                0.500000   \n",
       "3                  0.000000                0.500000   \n",
       "4                  0.136364                0.045455   \n",
       "\n",
       "   abs_title_sentiment_polarity  shares  \n",
       "0                      0.187500     593  \n",
       "1                      0.000000     711  \n",
       "2                      0.000000    1500  \n",
       "3                      0.000000    1200  \n",
       "4                      0.136364     505  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data frame by type: training, validation, and test\n",
    "df['type'] = ''\n",
    "df.loc[:int(train_pct * len(df)), 'type'] = 'train'\n",
    "df.loc[int(train_pct * len(df)) : int(val_pct * len(df)), 'type'] = 'val'\n",
    "df.loc[int(val_pct * len(df)):, 'type'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting columns into training, validation, and test data\n",
    "X_train = np.array(df[df.type == 'train'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_train = np.log(df[df.type == 'train'].shares).values.reshape((-1, 1))\n",
    "\n",
    "X_val = np.array(df[df.type == 'val'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_val = np.log(df[df.type == 'val'].shares).values.reshape((-1, 1))\n",
    "\n",
    "X_test = np.array(df[df.type == 'test'][[col for col in df.columns \\\n",
    "    if col not in ['url', 'shares', 'type']]])\n",
    "y_test = np.log(df[df.type == 'test'].shares).values.reshape((-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack a column of ones to the feature data\n",
    "# Use np.ones / np.ones_like to create a column of ones\n",
    "# Use np.hstack to stack the column to the matrix\n",
    "X_train = np.hstack((np.ones_like(y_train), X_train))\n",
    "X_val = np.hstack((np.ones_like(y_val), X_val))\n",
    "X_test = np.hstack((np.ones_like(y_test), X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert data to matrix\n",
    "X_train = np.matrix(X_train)\n",
    "y_train = np.matrix(y_train)\n",
    "X_val = np.matrix(X_val)\n",
    "y_val = np.matrix(y_val)\n",
    "X_test = np.matrix(X_test)\n",
    "y_test = np.matrix(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 1: RMSE vs lambda...\n",
      "==> Plotting completed.\n",
      "==> The optimal regularization parameter is  8.7286.\n"
     ]
    }
   ],
   "source": [
    "# STEP 1: RMSE vs lambda\n",
    "print('\\n==> Step 1: RMSE vs lambda...')\n",
    "# Fill in the code in linreg, findRMSE, and RMSE_vs_lambda\n",
    "reg_opt = RMSE_vs_lambda(X_train, y_train, X_val, y_val)\n",
    "print('==> The optimal regularization parameter is {reg: 4.4f}.'.format(\\\n",
    "    reg = reg_opt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal weights and bias for future use in step 3\n",
    "W_with_b_1 = linreg(X_train, y_train, reg = reg_opt)\n",
    "b_opt_1 = W_with_b_1[0]\n",
    "W_opt_1 = W_with_b_1[1: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> The RMSE on the validation set with the optimal regularization parameter is  0.8340.\n",
      "==> The RMSE on the test set with the optimal regularization parameter is  0.8628.\n",
      "==> Plotting completed.\n"
     ]
    }
   ],
   "source": [
    "# Report the RMSE with the found optimal weights on validation set\n",
    "val_RMSE = find_RMSE(W_with_b_1, X_val, y_val)\n",
    "print('==> The RMSE on the validation set with the optimal regularization parameter is {RMSE: 4.4f}.'.format(\\\n",
    "    RMSE=val_RMSE))\n",
    "\n",
    "# Report the RMSE with the found optimal weights on test set\n",
    "test_RMSE = find_RMSE(W_with_b_1, X_test, y_test)\n",
    "print('==> The RMSE on the test set with the optimal regularization parameter is {RMSE: 4.4f}.'.format(\\\n",
    "    RMSE=test_RMSE))\n",
    "\n",
    "# Norm vs lambda\n",
    "norm_vs_lambda(X_train, y_train, X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 3: Linear regression without bias...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Time elapsed for training: 4.12 seconds\n",
      "==> Difference in bias is  2.4734E-10\n",
      "==> Difference in weights is  3.6846E-10\n"
     ]
    }
   ],
   "source": [
    "# Part c\n",
    "# From here on, we will strip the columns of ones for all data\n",
    "X_train = X_train[:, 1:]\n",
    "X_val = X_val[:, 1:]\n",
    "X_test = X_test[:, 1:]\n",
    "# Fill in the code in linreg_no_bias\n",
    "# Compare the result with the one from step 1\n",
    "# The difference in norm should be a small scalar (i.e, 1e-10)\n",
    "print('\\n==> Step 3: Linear regression without bias...')\n",
    "b_opt_2, W_opt_2 = linreg_no_bias(X_train, y_train, reg = reg_opt)\n",
    "diff_bias = np.linalg.norm(b_opt_2 - b_opt_1)\n",
    "print('==> Difference in bias is {diff: 4.4E}'.format(diff = diff_bias))\n",
    "diff_W = np.linalg.norm(W_opt_2 -W_opt_1)\n",
    "print('==> Difference in weights is {diff: 4.4E}'.format(diff = diff_W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> Step 4: Gradient descent\n",
      "==> Running gradient descent...\n",
      "-- Iteration25 - training rmse  1.6604 - gradient norm  3.6435E+04\n",
      "-- Iteration50 - training rmse  1.2519 - gradient norm  2.4237E+04\n",
      "-- Iteration75 - training rmse  1.0664 - gradient norm  1.5202E+04\n",
      "-- Iteration100 - training rmse  0.9896 - gradient norm  9.7337E+03\n",
      "-- Iteration125 - training rmse  0.9596 - gradient norm  6.3025E+03\n",
      "-- Iteration150 - training rmse  0.9481 - gradient norm  4.1295E+03\n",
      "--Time elapsed for training: 24.07 seconds\n",
      "==> Plotting completed.\n",
      "==> Difference in bias is  1.5387E-01\n",
      "==> Difference in weights is  7.9875E-01\n"
     ]
    }
   ],
   "source": [
    "# Part e\n",
    "print('\\n==> Step 4: Gradient descent')\n",
    "b_gd, W_gd = grad_descent(X_train, y_train, X_val, y_val, reg = reg_opt)\n",
    "# Compare the result from the one from step 1\n",
    "diff_bias = np.linalg.norm(b_gd - b_opt_1)\n",
    "print('==> Difference in bias is {diff: 4.4E}'.format(diff = diff_bias))\n",
    "diff_W = np.linalg.norm(W_gd -W_opt_1)\n",
    "print('==> Difference in weights is {diff: 4.4E}'.format(diff = diff_W))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
