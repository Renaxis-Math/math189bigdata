{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==>Loading data...\n",
      "==>Data loaded succesfully.\n"
     ]
    }
   ],
   "source": [
    "import p2_data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "\t'''\n",
    "\t\tThe sigmoid function.\n",
    "\t'''\n",
    "\treturn 1. / (1. + np.exp(-x))\n",
    "\n",
    "def grad_logreg(X, y, W, reg = 0.0):\n",
    "\t'''\n",
    "\t\tReturn the gradient of W for logistic regression.\n",
    "\t'''\n",
    "\treturn X.T @ (sigmoid(X @ W) - y) + reg * W\n",
    "\n",
    "def newton_step(X, y, W, reg = 0.0):\n",
    "\t'''\n",
    "\tReturn the change of W according to Newton's method.\n",
    "\t'''\n",
    "\tmu = sigmoid(X @ W)\n",
    "\tg = grad_logreg(X, y, W, reg = reg)\n",
    "\tdiag = np.diag(np.squeeze(np.asarray(np.multiply(mu, 1. - mu))))\n",
    "\tH = X.T @ diag @ X + reg * np.eye(X.shape[1])\n",
    "\td = np.linalg.solve(H, g)\n",
    "\treturn d\n",
    "\n",
    "def NLL(X, y, W, reg = 0.0):\n",
    "\t'''\n",
    "\t\tCalculate negative log likelihood.\n",
    "\t'''\n",
    "\tmu = sigmoid(X @ W)\n",
    "\ttemp = np.multiply(y, np.log(mu)) + np.multiply((1. - y), np.log(1. - mu))\n",
    "\tnll = -sum(temp) + reg / 2 * np.linalg.norm(W) ** 2\n",
    "\treturn nll.item(0)\n",
    "\n",
    "def grad_descent(X, y, reg = 0.0, lr = 1e-4, eps = 1e-6, \\\n",
    "\tmax_iter = 500, print_freq = 20):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\t\tlr is the learning rate.\n",
    "\t\teps is the threshold of the norm for the gradients.\n",
    "\t\tmax_iter is the maximum number of iterations.\n",
    "\t\tprint_freq is the frequency of printing the report.\n",
    "\n",
    "\t\tReturn the optimal weight by gradient descent and\n",
    "\t\tthe corresponding learning objectives.\n",
    "\t'''\n",
    "\tm, n = X.shape\n",
    "\tnll_list = []\n",
    "\tW = np.zeros((n, 1))\n",
    "\tW_grad = np.ones_like(W)\n",
    "\titer_num = 0\n",
    "\tt_start = time.time()\n",
    "\twhile iter_num < max_iter and np.linalg.norm(W_grad) > eps:\n",
    "\t\tnll = NLL(X, y, W, reg = reg)\n",
    "\t\tif np.isnan(nll):\n",
    "\t\t\tbreak\n",
    "\t\tnll_list.append(nll)\n",
    "\t\tW_grad = grad_logreg(X, y, W, reg = reg)\n",
    "\t\tW -= lr * W_grad\n",
    "\t\tif (iter_num + 1) % print_freq == 0:\n",
    "\t\t\tprint('-- Iteration {} - \\\n",
    "\t\t\t\tnegative log likelihood {: 4.4f}'.format(iter_num + 1, nll))\n",
    "\t\titer_num += 1\n",
    "\n",
    "\tt_end = time.time()\n",
    "\n",
    "\treturn W, nll_list\n",
    "\n",
    "def newton_method(X, y, reg = 0.0, eps = 1e-6, \\\n",
    "\tmax_iter = 20, print_freq = 5):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\t\teps is the threshold of the norm for the gradients.\n",
    "\t\tmax_iter is the maximum number of iterations.\n",
    "\t\tprint_freq is the frequency of printing the report.\n",
    "\n",
    "\t\tReturn the optimal weight by Netwon's method and the corresponding\n",
    "\t\tlearning objectives.\n",
    "\t'''\n",
    "\tm, n = X.shape\n",
    "\tnll_list = []\n",
    "\tW = np.zeros((n, 1))\n",
    "\tstep = np.ones_like(W)\n",
    "\tprint('==> Running Newton\\'s method...')\n",
    "\titer_num = 0\n",
    "\tt_start = time.time()\n",
    "\n",
    "\twhile iter_num < max_iter and np.linalg.norm(step) > eps:\n",
    "\t\tnll = NLL(X, y, W, reg = reg)\n",
    "\t\tif np.isnan(nll):\n",
    "\t\t\tbreak\n",
    "\t\tnll_list.append(nll)\n",
    "\n",
    "\t\tstep = newton_step(X, y, W, reg = reg)\n",
    "\t\tW -= step\n",
    "\t\tif (iter_num + 1) % print_freq == 0:\n",
    "\t\t\tprint('-- Iteration {} - \\\n",
    "\t\t\t\tnegative log likelihood {: 4.4f}'.format(iter_num + 1, nll))\n",
    "\t\titer_num += 1\n",
    "\n",
    "\tt_end = time.time()\n",
    "\n",
    "\treturn W, nll_list\n",
    "\n",
    "def predict(X, W):\n",
    "\t'''\n",
    "\t\tReturn the predicted labels.\n",
    "\t'''\n",
    "\tmu = sigmoid(X @ W)\n",
    "\treturn (mu >= 0.5).astype(int)\n",
    "\n",
    "def get_description(X, y, W):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\tW is the weight with dimension (n + 1) x 1.\n",
    "\n",
    "\t\tReturn the accuracy, precision, recall and F-1 score of the prediction.\n",
    "\t'''\n",
    "\t# YOUR CODE GOES BELOW\n",
    "\tm, n = X.shape\n",
    "\ty_pred = predict(X, W)\n",
    "\tcount_a, count_p, count_r = 0, 0, 0\n",
    "\ttotal_p, total_r = 0, 0\n",
    "\tfor index in range(m):\n",
    "\t\tactual, pred = y.item(index), y_pred.item(index)\n",
    "\t\tif actual == pred:\n",
    "\t\t\tcount_a += 1\n",
    "\t\tif actual == 1:\n",
    "\t\t\ttotal_r += 1\n",
    "\t\t\tif pred == 1:\n",
    "\t\t\t\tcount_r += 1\n",
    "\t\tif pred == 1:\n",
    "\t\t\ttotal_p += 1\n",
    "\t\t\tif actual == 1:\n",
    "\t\t\t\tcount_p += 1\n",
    "\taccuracy = 1. * count_a / m\n",
    "\tprecision = 1. * count_p / total_p\n",
    "\trecall = 1. * count_r / total_r\n",
    "\tf1 = 2. * precision * recall / (precision + recall)\n",
    "\treturn accuracy, precision, recall, f1\n",
    "\n",
    "def plot_description(X_train, y_train, X_test, y_test):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\n",
    "\t\tPlot accuracy/precision/recall/F-1 score versus lambda.\n",
    "\t\tReturn the lambda that maximizes accuracy.\n",
    "\t'''\n",
    "\treg_list = [0., 0.01, 0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 20.0, 30.0]\n",
    "\treg_list.sort()\n",
    "\ta_list = []\n",
    "\tp_list = []\n",
    "\tr_list = []\n",
    "\tf1_list = []\n",
    "\t# Run Newton's method or gradient descent\n",
    "\tfor index in range(len(reg_list)):\n",
    "\t\treg = reg_list[index]\n",
    "\t\tW_opt, obj = grad_descent(X_train, y_train, reg = reg, \\\n",
    "\t\t\tlr = 2e-4, print_freq = 100)\n",
    "\t\taccuracy, precision, recall, f1 = get_description(X_test, y_test, W_opt)\n",
    "\t\ta_list.append(accuracy)\n",
    "\t\tp_list.append(precision)\n",
    "\t\tr_list.append(recall)\n",
    "\t\tf1_list.append(f1)\n",
    "\n",
    "\t# Generate plots\n",
    "\ta_vs_lambda_plot, = plt.plot(reg_list, a_list)\n",
    "\tplt.setp(a_vs_lambda_plot, color = 'red')\n",
    "\tp_vs_lambda_plot, = plt.plot(reg_list, p_list)\n",
    "\tplt.setp(p_vs_lambda_plot, color = 'green')\n",
    "\tr_vs_lambda_plot, = plt.plot(reg_list, r_list)\n",
    "\tplt.setp(r_vs_lambda_plot, color = 'blue')\n",
    "\tf1_vs_lambda_plot, = plt.plot(reg_list, f1_list)\n",
    "\tplt.setp(f1_vs_lambda_plot, color = 'yellow')\n",
    "\tplt.legend((a_vs_lambda_plot, p_vs_lambda_plot, r_vs_lambda_plot, \\\n",
    "\t\tf1_vs_lambda_plot), ('accuracy', 'precision', 'recall', 'F-1'),\\\n",
    "\t\t loc = 'best')\n",
    "\tplt.title('Testing descriptions')\n",
    "\tplt.xlabel('regularization parameter')\n",
    "\tplt.ylabel('Metric')\n",
    "\tplt.savefig('hw4pr2a_description.png', format = 'png')\n",
    "\tplt.close()\n",
    "\n",
    "\t# Find the param that maximizes accuracy\n",
    "\topt_reg_index = np.argmax(a_list)\n",
    "\treg_opt = reg_list[opt_reg_index]\n",
    "\treturn reg_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.df_train; df_test = data.df_test\n",
    "X_train = data.X_train; y_train = data.y_train\n",
    "X_test = data.X_test; y_test = data.y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting data for logistic regression\n",
    "df_train_logreg = df_train[df_train.label <= 1]\n",
    "X_train_logreg = np.array(df_train_logreg[:][[col for \\\n",
    "    col in df_train_logreg.columns if col != 'label']]) / 256.\n",
    "y_train_logreg = np.array(df_train_logreg[:][['label']])\n",
    "df_test_logreg = df_test[df_test.label <= 1]\n",
    "X_test_logreg = np.array(df_test_logreg[:][[col for \\\n",
    "    col in df_test_logreg.columns if col != 'label']]) / 256.\n",
    "y_test_logreg = np.array(df_test_logreg[:][['label']])\n",
    "\n",
    "# stacking a column of 1's\n",
    "X_train_logreg = np.hstack((np.ones_like(y_train_logreg), X_train_logreg))\n",
    "X_test_logreg = np.hstack((np.ones_like(y_test_logreg), X_test_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent w/ Newton Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Iteration 20 - \t\t\t\tnegative log likelihood  137.0131\n",
      "-- Iteration 40 - \t\t\t\tnegative log likelihood  108.1078\n",
      "-- Iteration 60 - \t\t\t\tnegative log likelihood  92.1446\n",
      "-- Iteration 80 - \t\t\t\tnegative log likelihood  81.8943\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  74.7108\n",
      "-- Iteration 120 - \t\t\t\tnegative log likelihood  69.3697\n",
      "-- Iteration 140 - \t\t\t\tnegative log likelihood  65.2198\n",
      "-- Iteration 160 - \t\t\t\tnegative log likelihood  61.8818\n",
      "-- Iteration 180 - \t\t\t\tnegative log likelihood  59.1204\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  56.7824\n",
      "-- Iteration 220 - \t\t\t\tnegative log likelihood  54.7646\n",
      "-- Iteration 240 - \t\t\t\tnegative log likelihood  52.9949\n",
      "-- Iteration 260 - \t\t\t\tnegative log likelihood  51.4220\n",
      "-- Iteration 280 - \t\t\t\tnegative log likelihood  50.0080\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  48.7248\n",
      "-- Iteration 320 - \t\t\t\tnegative log likelihood  47.5507\n",
      "-- Iteration 340 - \t\t\t\tnegative log likelihood  46.4689\n",
      "-- Iteration 360 - \t\t\t\tnegative log likelihood  45.4660\n",
      "-- Iteration 380 - \t\t\t\tnegative log likelihood  44.5315\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  43.6566\n",
      "-- Iteration 420 - \t\t\t\tnegative log likelihood  42.8341\n",
      "-- Iteration 440 - \t\t\t\tnegative log likelihood  42.0581\n",
      "-- Iteration 460 - \t\t\t\tnegative log likelihood  41.3236\n",
      "-- Iteration 480 - \t\t\t\tnegative log likelihood  40.6262\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  39.9623\n",
      "==> Running Newton's method...\n",
      "-- Iteration 5 - \t\t\t\tnegative log likelihood  115.2795\n",
      "-- Iteration 10 - \t\t\t\tnegative log likelihood  1.2703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/pc/1h9xlpvd7rd5xtvc_7rkfznr0000gn/T/ipykernel_88421/2625235106.py:29: RuntimeWarning: divide by zero encountered in log\n",
      "  temp = np.multiply(y, np.log(mu)) + np.multiply((1. - y), np.log(1. - mu))\n",
      "/var/folders/pc/1h9xlpvd7rd5xtvc_7rkfznr0000gn/T/ipykernel_88421/2625235106.py:29: RuntimeWarning: invalid value encountered in multiply\n",
      "  temp = np.multiply(y, np.log(mu)) + np.multiply((1. - y), np.log(1. - mu))\n"
     ]
    }
   ],
   "source": [
    "W_gd, nll_list_gd = grad_descent(X_train_logreg, y_train_logreg, reg = 1e-6)\n",
    "W_newton, nll_list_newton = newton_method(X_train_logreg, y_train_logreg, \\\n",
    "    reg = 1e-6)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "nll_gd_plot, = plt.plot(range(len(nll_list_gd)), nll_list_gd)\n",
    "plt.setp(nll_gd_plot, color = 'red')\n",
    "nll_newton_plot, = plt.plot(range(len(nll_list_newton)), nll_list_newton)\n",
    "plt.setp(nll_newton_plot, color = 'green')\n",
    "plt.legend((nll_gd_plot, nll_newton_plot), ('Gradient descent', 'Newton\\'s method'), loc = 'best')\n",
    "plt.title('Convergence Plot on Binary MNIST Classification')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('NLL')\n",
    "plt.savefig('hw4pr2a_convergence.png', format = 'png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics Plot and get regularized parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  72.7286\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  48.1433\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  38.4131\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  32.8454\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  28.9308\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  72.8140\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  48.2281\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  38.5023\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  32.9410\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  29.0344\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  73.5791\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  48.9863\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  39.2980\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  33.7935\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  29.9580\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  76.9166\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  52.2491\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  42.6962\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  37.4201\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  33.8755\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  80.9476\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  56.0952\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  46.6530\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  41.6232\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  38.3985\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  88.5592\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  63.0885\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  53.7542\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  49.1614\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  46.4860\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  108.1521\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  79.7520\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  70.8768\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  67.6606\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  66.2037\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  132.1355\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  99.6438\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  93.6335\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  92.2492\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  91.7393\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  160.3170\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  132.6816\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  131.1149\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  130.8338\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  130.7545\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  178.3458\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  162.6599\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  162.1722\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  162.1023\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  162.0883\n",
      "Optimal regularization parameter is 5.0000\n"
     ]
    }
   ],
   "source": [
    "reg_opt = plot_description(X_train_logreg, y_train_logreg, X_test_logreg, y_test_logreg)\n",
    "print('Optimal regularization parameter is {:4.4f}'.format(reg_opt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem (b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import p2_data as data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLL(X, y, W, reg=0.0):\n",
    "\t'''\n",
    "\t\tCalculate negative log likelihood for softmax regression.\n",
    "\t'''\n",
    "\t# YOUR CODE GOES BELOW\n",
    "\tmu = X @ W # m x k\n",
    "\texp_mu = np.exp(mu)\n",
    "\tprob = exp_mu / exp_mu.sum(axis=1).reshape(-1, 1)\n",
    "\tgroundTruth = y * np.log(prob)\n",
    "\treturn -groundTruth.sum(axis=1).sum() + reg * np.diag(W.T @ W).sum()\n",
    "\n",
    "def grad_softmax(X, y, W, reg=0.0):\n",
    "\t'''\n",
    "\t\tReturn the gradient of W for softmax regression.\n",
    "\t'''\n",
    "\t# YOUR CODE BELOW\n",
    "\tmu = X @ W\n",
    "\texp_mu = np.exp(mu)\n",
    "\tprob = exp_mu / exp_mu.sum(axis=1).reshape(-1, 1)\n",
    "\treturn X.T @ (prob - y) + reg * W\n",
    "\n",
    "def predict(X, W):\n",
    "\t'''\n",
    "\t\tReturn y_pred with dimension m x 1.\n",
    "\t'''\n",
    "\t# YOUR CODE BELOW\n",
    "\tmu = X @ W\n",
    "\texp_mu = np.exp(mu)\n",
    "\tprob = exp_mu / exp_mu.sum(axis=1).reshape(-1, 1)\n",
    "\ty_pred = np.argmax(prob, axis=1).reshape(-1, 1)\n",
    "\treturn y_pred\n",
    "\n",
    "def get_accuracy(y_pred, y):\n",
    "\t'''\n",
    "\t\tReturn the percentage of same bits between y_pred and y.\n",
    "\t'''\n",
    "\tdiff = (y_pred == y).astype(int)\n",
    "\taccu = 1. * diff.sum() / len(y)\n",
    "\treturn accu\n",
    "\n",
    "def grad_descent(X, y, reg=0.0, lr=1e-5, eps=1e-6, \\\n",
    "\tmax_iter=500, print_freq=20):\n",
    "\t'''\n",
    "\t\tX is matrix with dimension m x (n + 1).\n",
    "\t\ty is label with dimension m x 1.\n",
    "\t\treg is the parameter for regularization.\n",
    "\t\tlr is the learning rate.\n",
    "\t\teps is the threshold of the norm for the gradients.\n",
    "\t\tmax_iter is the maximum number of iterations.\n",
    "\t\tprint_freq is the frequency of printing the report.\n",
    "\n",
    "\t\tReturn the optimal weight by gradient descent and\n",
    "\t\tthe corresponding learning objectives.\n",
    "\t'''\n",
    "\tm, n = X.shape\n",
    "\tk = y.shape[1]\n",
    "\tnll_list = []\n",
    "\t# initialize the weight and its gradient\n",
    "\tW = np.zeros((n, k))\n",
    "\tW_grad = np.ones((n, k))\n",
    "\tprint('==> Running gradient descent...')\n",
    "\titer_num = 0\n",
    "\tt_start = time.time()\n",
    "\t# Running the gradient descent algorithm\n",
    "\t# Update W\n",
    "\t# Calculate learning objectives\n",
    "\t# YOUR CODE GOES BELOW\n",
    "\twhile iter_num < max_iter and np.linalg.norm(W_grad) > eps:\n",
    "\t\t# calculate NLL\n",
    "\t\tnll = NLL(X, y, W, reg=reg)\n",
    "\t\tif np.isnan(nll):\n",
    "\t\t\tbreak\n",
    "\t\tnll_list.append(nll)\n",
    "\t\t# calculate gradients and update W\n",
    "\t\tW_grad = grad_softmax(X, y, W, reg=reg)\n",
    "\t\tW -= lr * W_grad\n",
    "\n",
    "\t\tif (iter_num + 1) % print_freq == 0:\n",
    "\t\t\tprint('-- Iteration {} - \\\n",
    "\t\t\t\tnegative log likelihood {: 4.4f}'.format(iter_num + 1, nll))\n",
    "\t\titer_num += 1\n",
    "\t# benchmark\n",
    "\tt_end = time.time()\n",
    "\tprint('-- Time elapsed for running gradient descent: {t:2.2f} \\\n",
    "\t\tseconds'.format(t=t_end - t_start))\n",
    "\n",
    "\treturn W, nll_list\n",
    "\n",
    "def accuracy_vs_lambda(X_train, y_train_OH, X_test, y_test, lambda_list):\n",
    "\t'''\n",
    "\t\tGenerate accuracy for all given regularization parameters.\n",
    "\t\tGenerate a plot of accuracy vs lambda.\n",
    "\t\tReturn the lambda with optimal accuracy.\n",
    "\t'''\n",
    "\t# Find corresponding accuracy values for each parameter\n",
    "\taccu_list = []\n",
    "\t# YOUR CODE BELOW\n",
    "\tfor reg in lambda_list:\n",
    "\t\tW, nll_list = grad_descent(X_train, y_train_OH, reg=reg, lr=2e-5, \\\n",
    "\t\tprint_freq=50)\n",
    "\t\ty_pred = predict(X_test, W)\n",
    "\t\taccuracy = get_accuracy(y_pred, y_test)\n",
    "\t\taccu_list.append(accuracy)\n",
    "\t\tprint('-- Accuracy is {:2.4f} for lambda = {:2.2f}'.format(accuracy, reg))\n",
    "\t# Plot accuracy vs lambda\n",
    "\tprint('==> Printing accuracy vs lambda...')\n",
    "\tplt.style.use('ggplot')\n",
    "\tplt.plot(lambda_list, accu_list)\n",
    "\tplt.title('Accuracy versus Lambda in Softmax Regression')\n",
    "\tplt.xlabel('Lambda')\n",
    "\tplt.ylabel('Accuracy')\n",
    "\tplt.savefig('hw4pr2b_lva.png', format = 'png')\n",
    "\tplt.close()\n",
    "\tprint('==> Plotting completed.')\n",
    "\t# Find optimal lambda\n",
    "\topt_lambda_index = np.argmax(accu_list)\n",
    "\treg_opt = lambda_list[opt_lambda_index]\n",
    "\treturn reg_opt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = data.df_train\n",
    "df_test = data.df_test\n",
    "'''\n",
    "    X is a matrix with dimension m x n\n",
    "    y is a vector with dimension m x 1\n",
    "'''\n",
    "X_train = data.X_train\n",
    "y_train = data.y_train\n",
    "X_test = data.X_test\n",
    "y_test = data.y_test\n",
    "# stacking an array of ones\n",
    "X_train = np.hstack((np.ones_like(y_train), X_train))\n",
    "X_test = np.hstack((np.ones_like(y_test), X_test))\n",
    "# one hot encoder\n",
    "enc = OneHotEncoder()\n",
    "y_train_OH = enc.fit_transform(y_train.copy()).astype(int).toarray()\n",
    "y_test_OH = enc.fit_transform(y_test.copy()).astype(int).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  22498.2902\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20132.7688\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  19074.1782\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  18420.1137\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  17962.7355\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  17618.8274\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  17347.3245\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  17125.3589\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  16939.0725\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  16779.5179\n",
      "-- Time elapsed for running gradient descent: 76.66 \t\tseconds\n",
      "-- Accuracy is 0.9221 for lambda = 0.01\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  22502.8732\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20138.7176\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  19081.3517\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  18428.3600\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  17971.9312\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  17628.8833\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  17358.1724\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  17136.9449\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  16951.3528\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  16792.4560\n",
      "-- Time elapsed for running gradient descent: 74.61 \t\tseconds\n",
      "-- Accuracy is 0.9221 for lambda = 0.10\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  22523.2392\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20165.1426\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  19113.2021\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  18464.9626\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  18012.7369\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  17673.4935\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  17406.2827\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  17188.3147\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  17005.7862\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  16849.7900\n",
      "-- Time elapsed for running gradient descent: 75.10 \t\tseconds\n",
      "-- Accuracy is 0.9221 for lambda = 0.50\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  22548.6899\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20198.1415\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  19152.9417\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  18510.6073\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  18063.5966\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  17729.0666\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  17466.1861\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  17252.2446\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  17073.4952\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  16921.0723\n",
      "-- Time elapsed for running gradient descent: 74.62 \t\tseconds\n",
      "-- Accuracy is 0.9220 for lambda = 1.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  23005.6340\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20787.0261\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  19854.6030\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  19312.1599\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  18952.0004\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  18694.7752\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  18501.8541\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  18352.0125\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  18232.5582\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  18135.4029\n",
      "-- Time elapsed for running gradient descent: 65.75 \t\tseconds\n",
      "-- Accuracy is 0.9221 for lambda = 10.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  25022.0136\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  23455.9916\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  22696.5373\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  22473.2174\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  22372.8437\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  22329.8237\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  22316.7414\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  22319.9960\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  22332.3015\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  22349.5392\n",
      "-- Time elapsed for running gradient descent: 78.18 \t\tseconds\n",
      "-- Accuracy is 0.9204 for lambda = 50.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  27567.2117\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  26397.3848\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  25977.7161\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  25797.0749\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  25776.7473\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  25821.9679\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  25876.8902\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  25928.0833\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  25973.4769\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  26012.9721\n",
      "-- Time elapsed for running gradient descent: 74.44 \t\tseconds\n",
      "-- Accuracy is 0.9191 for lambda = 100.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  32995.2904\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  32659.8978\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  32531.6174\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  32450.8015\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  32410.7265\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  32404.8881\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  32428.2569\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  32470.8874\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  32518.3967\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  32557.1261\n",
      "-- Time elapsed for running gradient descent: 73.97 \t\tseconds\n",
      "-- Accuracy is 0.9071 for lambda = 200.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  49477.5714\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  49153.4014\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  49261.1278\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  49280.3399\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  49299.0346\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  49314.9468\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  49321.5028\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  49324.8655\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  49326.9286\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  49327.9820\n",
      "-- Time elapsed for running gradient descent: 73.96 \t\tseconds\n",
      "-- Accuracy is 0.8575 for lambda = 500.00\n",
      "==> Running gradient descent...\n",
      "-- Iteration 50 - \t\t\t\tnegative log likelihood  68941.3363\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  69147.4186\n",
      "-- Iteration 150 - \t\t\t\tnegative log likelihood  68933.8354\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  68179.9928\n",
      "-- Iteration 250 - \t\t\t\tnegative log likelihood  69072.6269\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  69869.0004\n",
      "-- Iteration 350 - \t\t\t\tnegative log likelihood  68810.7251\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  67956.2604\n",
      "-- Iteration 450 - \t\t\t\tnegative log likelihood  70227.1391\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  69502.8560\n",
      "-- Time elapsed for running gradient descent: 78.46 \t\tseconds\n",
      "-- Accuracy is 0.8021 for lambda = 1000.00\n",
      "==> Printing accuracy vs lambda...\n",
      "==> Plotting completed.\n",
      "-- Optimal regularization parameter is 0.01\n",
      "==> Running gradient descent...\n",
      "-- Iteration 100 - \t\t\t\tnegative log likelihood  20132.7688\n",
      "-- Iteration 200 - \t\t\t\tnegative log likelihood  18420.1137\n",
      "-- Iteration 300 - \t\t\t\tnegative log likelihood  17618.8274\n",
      "-- Iteration 400 - \t\t\t\tnegative log likelihood  17125.3589\n",
      "-- Iteration 500 - \t\t\t\tnegative log likelihood  16779.5179\n",
      "-- Iteration 600 - \t\t\t\tnegative log likelihood  16518.1123\n",
      "-- Iteration 700 - \t\t\t\tnegative log likelihood  16310.5114\n",
      "-- Iteration 800 - \t\t\t\tnegative log likelihood  16139.7897\n",
      "-- Iteration 900 - \t\t\t\tnegative log likelihood  15995.7067\n",
      "-- Iteration 1000 - \t\t\t\tnegative log likelihood  15871.6418\n",
      "-- Iteration 1100 - \t\t\t\tnegative log likelihood  15763.0929\n",
      "-- Iteration 1200 - \t\t\t\tnegative log likelihood  15666.8749\n",
      "-- Iteration 1300 - \t\t\t\tnegative log likelihood  15580.6604\n",
      "-- Iteration 1400 - \t\t\t\tnegative log likelihood  15502.7035\n",
      "-- Iteration 1500 - \t\t\t\tnegative log likelihood  15431.6646\n",
      "-- Time elapsed for running gradient descent: 221.44 \t\tseconds\n"
     ]
    }
   ],
   "source": [
    "lambda_list = [0.01, 0.1, 0.5, 1.0, 10.0, 50.0, 100.0, 200.0, 500.0, 1000.0]\n",
    "reg_opt = accuracy_vs_lambda(X_train, y_train_OH, X_test, y_test, lambda_list)\n",
    "print('-- Optimal regularization parameter is {:2.2f}'.format(reg_opt))\n",
    "\n",
    "W_gd, nll_list_gd = grad_descent(X_train, y_train_OH, reg=reg_opt, max_iter=1500, lr=2e-5, \\\n",
    "    print_freq=100)\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "# Plot the learning curve of NLL vs Iteration\n",
    "# YOUR CODE BELOW\n",
    "nll_gd_plot, = plt.plot(range(len(nll_list_gd)), nll_list_gd)\n",
    "plt.setp(nll_gd_plot, color = 'red')\n",
    "plt.title('Convergence Plot on Softmax Regression with $\\lambda = {:2.2f}$'.format(reg_opt))\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('NLL')\n",
    "plt.savefig('hw4pr2b_convergence.png', format = 'png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
